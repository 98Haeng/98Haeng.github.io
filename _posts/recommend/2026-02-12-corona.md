---
title: "[Paper Review] CORONA: A Coarse-to-Fine Framework for Graph-based Recommendation with Large Language Models (SIGIR 2025)"
excerpt: "SIGIR 2025에서 공개된 CORONA 논문에 대한 리뷰입니다."

categories:
  - recommend
tags:
  - [recommend]

permalink: /recommend/corona/

toc: true
toc_sticky: true

date: 2026-02-12
last_modified_at: 2026-02-12
---

## Problem Statement and Previous Limitations
LLM 기반 추천에서 LLM을 사용하는 방법은 크게 2가지 방법으로 나뉘는데, 후보 선택을 하기 이전에 LLM을 적용하는지, 혹은 후보 선택을 한 이후에 추천을 LLM이 진행하는지로 나뉩니다. 

<img width="484" height="129" alt="image" src="https://github.com/user-attachments/assets/68bf9e9a-c3cd-4302-914b-717d308ef132" />

논문의 순서대로 후보 필터링 이후에 LLM을 사용하는 경우는 기존의 Traditional 추천 모델(예: SASRec 등)이 먼저 후보 아이템 집합을 생성하고, 이후 LLM이 후보들을 프롬프트에 입력받아 최종 예측 또는 리랭킹을 수행합니다. 이 방식은 프롬프트에 후보 목록을 포함해야 하므로 컨텍스트 길이(context-length)에 강하게 의존하며, LLM이 전체 아이템 공간을 직접적으로 다루기 어렵습니다. 또한 추천 성능이 후보 생성 단계의 Traditional RS 모델 품질에 크게 좌우되어, Traditional RS 모델에 대한 의존도 문제가 발생합니다.

<img width="463" height="150" alt="image" src="https://github.com/user-attachments/assets/3b7440ef-d34a-4460-9409-2e5d9d54ab65" />

다음으로 후보 필터링 이전에 LLM을 사용하는 방식은 LLM이 아이템 데이터(Description, Category 등)를 증강하거나 정제하는 전처리 역할을 수행하고, 이후 Traditional RS 모델이 해당 정보를 활용하여 예측 및 추천을 진행하는 형태입니다. 이 경우 LLM은 아이템 표현을 개선하는 데 기여할 수는 있으나, 추천 후보를 직접적으로 생성하거나 필터링하는 과정에는 개입하지 못하는 한계가 존재합니다.

이러한 2개 관점 각각의 문제를 해결하기 위해, LLM이 단순히 후보를 ‘후처리’하거나 ‘전처리’하는 것만이 아닌, 후보 필터링 과정 자체에 LLM을 포함하여 탐색 공간을 직접 줄이는 방법이 필요합니다.


## Method : CORONA - Chain Of Retrieval ON grAphs

<img width="1120" height="500" alt="image" src="https://github.com/user-attachments/assets/78fe5458-2a85-419c-b9ec-2816998a8ac2" />
<img width="650" height="175" alt="image" src="https://github.com/user-attachments/assets/544ab1ad-230b-4b86-8379-281f59ab7199" />

위의 문제를 해결하기 위해서 본 논문에서는 CORONA : Chain Of Retrieval ON grAphs를 제안합니다. CORONA는 LLM의 추론 능력을 활용하여 추천 정확도를 높이기 위해, user-item interaction graph 상에서 탐색 범위를 단계적으로 축소하는 방법을 활용합니다. 그 과정에서 LLM이 세밀하게 특정 아이템을 고르는 것 보다, 사용자의 선호도와 의도를 coarse-grained (넓은 범위) 수준에서 추론하는 것에 사용합니다. 이후 최종 아이템 추천은 GNN 기반의 message passing 방법으로 추천을 진행합니다.


CORONA는 전반적으로 다음의 사용자-아이템 상호작용 그래프를 기반으로 수행됩니다.

- 상호작용 그래프를 $\mathcal{G} = (\mathcal{U}, \mathcal{V}, \mathcal{A})$로 정의합니다.  
  - $\mathcal{U}$: 사용자 집합  
  - $\mathcal{V}$: 아이템 집합  
  - $\mathcal{A}\in\{0,1\}$: 상호작용 여부를 나타내는 인접 행렬(1이면 상호작용 있음, 0이면 없음)
- 입력으로는 사용자 프로필 및 사용자 이력(아이템/메타데이터)을 사용하며, 출력으로는 **2단계에 걸쳐 점진적으로 축소된 서브그래프**를 얻습니다.


### Method 1 : Preference-assisted Retrieval
Preference-assisted Retrieval 단계에서는 사용자 프로필을 기반으로 LLM이 **잠재 선호(preference)**를 추론하고, 이를 검색에 활용하여 서브그래프를 구성합니다.

이 과정에서는
- Input : 사용자 프로필(예: 나이, 성별 등 관련 정보)
- Output : 선호도를 반영한 서브그래프, 선호도 텍스트 요약

의 결과를 활용합니다.

Preference-assisted Retrieval 단계에서는 타겟 사용자의 프로필 정보를 자연어 instruction 형태로 구성하여 LLM에 입력하고, LLM이 사용자에 대한 잠재 선호도를 텍스트로 추론하도록 합니다.
이후 LLM이 출력한 선호도 텍스트를 임베딩 모델로 변환하여 선호도 쿼리 임베딩을 생성하고, 이 임베딩을 기반으로 그래프 내에서 유사한 사용자를 찾는 검색을 수행합니다. 이 과정에서 임베딩 모델은 `text-embedding-ada-002`를 사용합니다.

<img width="451" height="208" alt="image" src="https://github.com/user-attachments/assets/77150258-5368-4ce2-a335-9c247f3710c4" />

이때 CORONA는 타겟 사용자 $\mathcal{u}$와 다른 사용자 $\mathcal{u'}$ 사이의 hop 거리를 계산하여 $\text{dist}(\mathcal{u}, \mathcal{u'})$를 구분하고, 그래프상 가까운 사용자일수록 관심사가 비슷할 가능성이 높다는 가정을 반영합니다. 

<img width="421" height="83" alt="image" src="https://github.com/user-attachments/assets/febd05ba-2538-4bbc-bfab-14c1b837688e" />

구체적으로는 해당 거리 정보를 사용자 특징에 distance encoding 형태로 결합한 뒤, 선호도 쿼리 임베딩과 각 사용자 표현 $X_{u'}$ 사이의 코사인 유사도를 계산하여 Top-$k$ 사용자 집합을 선택합니다. 그리고 선택된 Top-$k$ 사용자들과 연결된 모든 아이템을 모아 1차 서브그래프를 구성합니다. 

<img width="449" height="190" alt="image" src="https://github.com/user-attachments/assets/5e12b0c1-d4ba-4412-8371-a4ec400852bb" />

마지막으로, 이 서브그래프에서 얻은 아이템 집합을 바탕으로 빈도가 높은 상위 $k$개 (20개) 속성 정보를 추출해 자연어 요약 텍스트를 만들고, 이를 다음 단계의 입력으로 사용합니다.


### Method 2 : Intent-assisted Retrieval

Intent-assisted Retrieval 단계에서는 이전 단계에서 얻은 서브그래프 및 요약 텍스트, 그리고 타겟 사용자의 최근 상호작용 이력을 바탕으로 LLM이 **의도(intent)**를 추론하고, 이를 통해 탐색 범위를 한 번 더 축소하는 과정을 수행합니다..

이 과정에서는
- Input : Stage 1에서 생성된 선호도 요약 텍스트, 타겟 사용자의 최근 상호작용 이력(아이템 메타데이터 포함)
- Output: 선호도와 의도가 반영된 더 작은 최종 서브그래프
을 사용 및 도출합니다.

<img width="443" height="269" alt="image" src="https://github.com/user-attachments/assets/b19bf0fb-b09e-4622-9841-9db6c11c46f1" />

다음 Intent-assisted Retrieval 단계에서는 1단계에서 얻은 서브그래프의 요약 텍스트와 타겟 사용자의 최근 상호작용 이력(아이템 메타데이터 포함)을 함께 LLM에 입력하여, 사용자가 현재 시점에서 무엇을 원하는지에 해당하는 의도를 추론합니다. 
LLM이 출력한 의도 텍스트(Intent)는 다시 임베딩되어 의도 쿼리 임베딩으로 변환되며, 이 임베딩을 이용해 1단계 서브그래프를 한 번 더 축소합니다. 
이 과정에서 CORONA는 검색 범위를 전체 사용자 공간으로 확장하지 않고, 1단계에서 이미 선택된 사용자 집합 내부로 제한하여 탐색 비용을 줄이고 개인화를 강화합니다. 즉, 1단계에서 확보한 후보 사용자들 중에서 의도 쿼리 임베딩과의 유사도(코사인 유사도)를 다시 계산해 Top-$\frac{k}{2}$ 사용자를 선택하고, 이 사용자들과 연결된 아이템들을 포함하여 최종 서브그래프를 구성합니다. 
또한 두 단계의 retrieval이 정답 사용자 $\mathcal{u'}$를 더 잘 찾도록, 선호도 쿼리 $E_{Q1}^T$와 의도 쿼리 $E_{Q2}^T$가 동일한 Retriever 표현 $X_{u'}$에 대해 일관되게 높은 점수를 갖도록 InfoNCE 기반의 공동 손실로 학습합니다. 이를 통해 선호 기반 검색과 의도 기반 검색이 단절되지 않고 “선호 → 의도”로 이어지는 검색 체인이 안정적으로 형성되도록 유도합니다.

여기에서  서브그래프 구축을 위한 InfoNCE 기반의 공동의 Loss Function을 사용합니다. Preference, Intent 두 정보를 모두 반영하여 선호도 쿼리와 의도 쿼리가 모두 올바른 사용자 $\mathcal{u'}$를 가깝게 찾도록, Retriever를 공동으로 학습시키는 InfoNCE 기반 손실을 사용합니다.
<img width="1192" height="164" alt="image" src="https://github.com/user-attachments/assets/6e133372-a190-4058-98d2-980d50baf3f3" />

여기서 사용되는 정보는
- $E_{Q1}^T$: 선호도(preference) 쿼리 임베딩  
- $E_{Q2}^T$: 의도(intent) 쿼리 임베딩  

학습 목표는 정답 사용자 $\mathcal{u'}$에 대한 점수가 높아지도록, 동일한 Retriever 표현 $X_{u'}$를 일관되게 학습시키는 것입니다.


### Method 3 : GNN-enhanced Retrieval

마지막 단계에서는 Intent-assisted Retrieval로 구성된 최종 서브그래프 위에서 GNN message passing을 수행하여 협업 정보를 강화하고, 최종 아이템 추천을 수행합니다.

여기에서 Input, Output으로는 
- Input: 최종 서브그래프
- Output: 다음에 추천할 아이템 Top-$n$
입니다.

마지막 GNN-enhanced Retrieval 단계에서는 2단계에서 구축된 최종 서브그래프 위에서 GNN 메시지 패싱을 수행하여 협업 정보를 강화하고, 최종 추천 점수를 계산합니다. 구체적으로는 최종 서브그래프를 기반으로 타겟 사용자 임베딩 $H_u^T$를 생성한 뒤, 서브그래프 내 각 아이템 표현 $M_v$와의 inner product로 점수를 산출합니다. 
<img width="241" height="40" alt="image" src="https://github.com/user-attachments/assets/c02e8c82-ca90-41f5-a3a3-2e97ad600b62" />

학습은 BPR Loss를 기반으로 진행되며, negative 아이템은 최종 서브그래프에 포함된 아이템 집합에서 랜덤 샘플링합니다. 최종적으로 점수가 높은 아이템을 Top-$n$으로 선택하여 추천 결과로 출력합니다. 결과적으로 CORONA는 LLM을 통해 사용자의 선호와 의도를 단계적으로 추론하고, 이를 임베딩 기반 검색으로 연결해 그래프 탐색 범위를 좁힌 뒤, 축소된 서브그래프에서 GNN을 적용하여 협업 신호를 반영함으로써 추천 성능을 향상시키는 방법이라고 정리할 수 있습니다.

<img width="435" height="70" alt="image" src="https://github.com/user-attachments/assets/3b5a988b-b42a-41d0-800d-208e47474aab" />


## Experiments

### Experiments Settings

본 연구에서는 CORONA의 성능을 검증하기 위해 Netflix, MovieLens, Amazon-book 데이터셋을 사용하여 실험을 수행하였습니다. 

사용자 선호도 및 의도 추론을 위한 LLM으로는 GPT-4o-mini를, LLM이 생성한 텍스트(선호/의도)를 임베딩하여 검색 쿼리로 활용하기 위해 text-embedding-ada-002 임베딩 모델을 사용했습니다. 또한 텍스트 정보를 직접 활용하기 어려운 베이스라인(예: LightGCN)의 경우에는, 비교의 공정성을 위해 텍스트 인코딩 결과를 노드 feature로 주입하는 방식으로 실험 설정을 맞추어 성능을 비교하였습니다. 최종 추천 단계의 그래프 모델은 2-layer GCN을 사용하였고, 이 과정에서 hidden size는 128로 설정하였습니다. 데이터 분할은 leave-one-out 방식으로 수행하였으며, 평가는 Recall@K와 NDCG@K를 사용하되 $K \in \{10, 20, 50\}$에 대해 측정하였습니다. 또한 모든 아이템을 대상으로 하는 full ranking 환경에서 평가를 진행하고, 결과의 안정성을 위해 5회 반복 실험의 평균값을 보고하였습니다.

### Main Results

<img width="1084" height="381" alt="image" src="https://github.com/user-attachments/assets/4caef394-4afd-41cb-8ffc-444660670c40" />

메인 성능 비교 결과, CORONA는 실험에 포함된 모든 비교 모델들보다 일관되게 높은 추천 성능을 보였습니다. 특히 베이스라인 대비 최소 16% 이상의 성능 향상이 관찰되었으며, 이는 LLM을 후보 필터링(서브그래프 축소) 단계에 포함시킴으로써 그래프 탐색 범위가 효과적으로 줄어들고, 결과적으로 더 적합한 후보 아이템 집합을 구성할 수 있었기 때문이라고 해석할 수 있습니다. 즉, CORONA는 LLM을 단순 보조적 전처리 도구로 두는 것이 아니라, 후보 축소 과정의 핵심 구성요소로 사용했을 때 실질적인 성능 이득이 발생함을 보여줍니다.

### Cold-start Results

<img width="924" height="277" alt="image" src="https://github.com/user-attachments/assets/3d072c6b-2184-4010-97f2-4f1b92966848" />

Cold-start 환경에서는 아이템 상호작용 수가 2개 이하인 아이템들만을 대상으로 평가를 수행하였습니다. 이 설정에서 CORONA는 cold-start 특화 모델로 알려진 LLM-ins보다도 더 높은 지표를 보였으며, 이는 데이터 희소성이 큰 상황에서 텍스트 정보를 활용하는 전략이 전통적인 추천 모델보다 유리할 수 있음을 시사합니다. 특히 CORONA는 LLM의 추론을 통해 사용자의 선호 및 의도를 더 풍부한 텍스트 신호로 정리하고, 이를 기반으로 검색 쿼리를 구성하여 후보 공간을 정제하기 때문에, 상호작용이 부족한 아이템이 포함된 상황에서도 희소성의 영향을 완화하는 데 도움이 된다고 설명할 수 있습니다.

### Ablation Study

### Combinations of Different Subgraph Retrieval Methods and GNNs

<img width="1656" height="484" alt="image" src="https://github.com/user-attachments/assets/6e0800ce-8a12-4c55-9427-7bf7a02ddc1f" />

서브그래프를 포착하는 방식과 최종 GNN 조합이 성능에 미치는 영향을 분석하기 위해, 다양한 retrieval 방법 및 GNN 조합을 비교하는 ablation을 수행하였습니다. 

그 결과, full graph 전체에 대해 GNN을 수행하는 방식보다, CORONA처럼 LLM 기반 retrieval을 통해 먼저 서브그래프를 구성한 뒤 그 위에서 GNN 메시지 패싱을 수행하는 방식이 훨씬 더 높은 성능을 보였습니다. 이는 후보 공간을 먼저 정제하여 노이즈를 줄인 상태에서 협업 신호를 반영하는 것이 효과적임을 의미합니다.

### Ablation of Different Components

<img width="1604" height="420" alt="image" src="https://github.com/user-attachments/assets/d2854249-1ec8-483e-b827-57fa65ae77e0" />

CORONA의 구성 요소별 기여도를 확인하기 위해 다음과 같은 제거 실험을 수행하였습니다.

먼저 w/o Preference-assisted Retrieval은 1단계(선호 기반) retrieval을 제거한 설정이며, w/o Intent-assisted Retrieval은 2단계(의도 기반) retrieval을 제거한 설정입니다. 또한 w/o GNN-enhanced Retrieval은 마지막 GNN 메시지 패싱 단계를 제거한 설정입니다. 한편 “Reasoning”의 효과를 확인하기 위해, w/o Preference Reasoning에서는 LLM이 선호를 추론하지 않고 사용자 프로필을 그대로 임베딩해 Q1을 구성하였고, w/o Intent Reasoning에서는 LLM이 의도를 추론하지 않고 사용자 히스토리를 그대로 임베딩해 Q2를 구성하였습니다. 

실험 결과, 선호도 추론과 의도 추론이 제거되었을 때 성능 하락 폭이 가장 크게 나타났으며, 이는 CORONA에서 LLM의 “추론을 통한 요약/정제”가 단순 임베딩 기반 구성보다 검색 및 후보 축소 품질에 더 큰 영향을 미친다는 점을 보여줍니다.

### Influence of Different LLMs

<img width="808" height="520" alt="image" src="https://github.com/user-attachments/assets/c20798c4-901e-4d4d-9369-3b4f6da59a53" />

LLM의 종류에 따른 영향을 분석하기 위해, 메인 실험에서 사용한 GPT-4o-mini와 비교 대상으로 Vicuna-7B-v1.5를 사용하여 성능을 비교하였습니다. 

비교 결과 GPT 계열이 더 높은 성능을 보였으며, 저자들은 이를 “더 강한 reasoning 능력”이 더 큰 추천 성능 이득으로 연결된 결과로 해석합니다. 
즉, CORONA 구조에서는 LLM의 추론 품질이 곧 검색 쿼리 품질로 이어지고, 이는 최종 후보 정제 및 추천 성능에 직접적인 영향을 준다고 정리할 수 있습니다.

## Hyperparameter Analysis

<img width="838" height="536" alt="image" src="https://github.com/user-attachments/assets/e16adb41-c427-4a45-8bdf-d03e57a078dd" />

하이퍼파라미터 분석에서는 서브그래프 크기를 결정하는 $k$ 값, LLM temperature, distance encoding dimension, 그리고 GNN hidden size에 대한 민감도를 평가하였습니다. 
먼저 $k$를 1000부터 4000까지 변화시키며 실험한 결과, $k$가 증가할수록 성능이 상승하다가 특정 지점 이후 다시 하락하는 경향이 나타났고, 최대 성능은 $k=3000$ 부근에서 관찰되었습니다. 이는 $k$가 너무 작으면 유효 후보가 충분히 포함되지 못하고, $k$가 너무 크면 후보가 과도하게 늘어나 노이즈가 증가할 수 있기 때문으로 설명됩니다. 
또한 LLM temperature는 초기에는 증가에 따라 성능이 개선되지만, 일정 수준 이후에는 추론 텍스트의 일관성이 흔들리면서 쿼리 임베딩이 불안정해지고, 그 결과 필터링 품질이 저하되어 성능이 하락하는 경향이 나타났습니다.
거리 인코딩 차원(distance encoding dimension)의 경우 차원이 너무 작으면 거리 구분이 약해지고, 너무 크면 과적합 위험이 커질 수 있어, 실험에서는 2 정도가 적정한 설정으로 보고되었습니다. 
마지막으로 GNN hidden size는 커질수록 표현력이 증가하여 성능이 개선되는 경향이 있으나, 실험에서는 128에서 가장 좋은 결과가 보고되었고, 너무 작으면 표현력이 부족하며 너무 크면 과적합 등의 문제로 성능이 다시 하락하는 패턴이 관찰되었습니다.

## Efficiency Analysis

<img width="810" height="522" alt="image" src="https://github.com/user-attachments/assets/a8a04d0a-a7b2-4c68-a0db-c33a9d07c8d7" />

효율성 분석에서는 CORONA와 다른 추천 모델 3종을 비교하여, 시간 및 비용 측면의 특성을 확인했습니다. 저자들은 CORONA가 대규모 추론을 수행하는 대상이 주로 “사용자”이며, 아이템이나 전체 상호작용에 대해 대규모 추론을 반복적으로 수행하지 않기 때문에, 동일 조건에서 다른 방식 대비 효율적일 수 있다고 주장합니다. 
다만 GPT-4o-mini를 사용하는 경우에는 네트워크 호출로 인한 오버헤드가 존재하므로, 로컬에서 동작하는 LLM 기반 방식이 순수 실행 속도 측면에서는 더 빠를 수 있다는 점도 함께 언급됩니다. 결과적으로 CORONA는 추론의 적용 범위를 후보 축소에 필요한 핵심 지점으로 제한함으로써 비용 대비 효과를 확보하려는 설계라고 정리할 수 있습니다.


## Conclusion
### Contribution

CORONA는 LLM 기반 추천에서 LLM이 후보 선택 과정 혹은 추천에서만 활용하는 것이 아닌, LLM의 추론 능력을 최대로 활용해서 추천 성능을 끌어올렸다는 점에서 의의가 있습니다. 또한, 사용자의 선호도, 이전 상호작용 아이템에 기반한 의도를 파악하여 추천을 진행한다는 점에서, 이전 정보들을 효과적으로 활용하는 것을 알 수 있습니다.

### Limitation

Contribution에 반해, CORONA는 LLM을 Multi-call하는 방식으로 사용합니다. 거기에 추천은 GNN 모델로 따로 진행을 하기에, 시간 부분에서는 효율적일 수 있지만, 비용이 많이 들 수 있다는 문제가 있습니다 (VRAM 등). 
