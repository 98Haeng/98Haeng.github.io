---
title: "[Paper Review] Lost in Sequence : Do Large Language Models Understand Sequential Recommendation? (KDD 2025)"
excerpt: "LLM-SRec 논문을 제안하는 Lost in Sequence 논문에 대한 리뷰입니다."

categories:
  - recommend
tags:
  - [recommend]

permalink: /recommend/llmsrec/

toc: true
toc_sticky: true

date: 2026-01-24
last_modified_at: 2026-01-24
---

## Problem  Statement

### LLM-based recommendation (LLM4Rec)

<img width="1280" height="539" alt="image" src="https://github.com/user-attachments/assets/202f856b-6ad7-4eda-838b-df92ac440664" />


- 이전 상호작용 정보를 LLM에 입력하여 다음 아이템을 추천하는 방식
- 순차 정보보다는 주로 텍스트 정보에 의존하는 추천 시스템 → Cold-start에는 강력하지만, Warm-start에는 약한 문제점을 가지고 있음.
- 최근 LLM 기반 추천의 트렌드로는 Collaborative information을 LLM에 반영하는 추천 모델이 연구되고 있음.
- 대표적인 이전 모델 : TALLRec, LLaRA, CoLLM, A-LLMRec

### Previous Limitations

- LLM-based recommendation 모델에서 순차 정보가 간과되고 있음. (순차 정보에 대한 이해 부족)
    
    → LLM에 순차 정보를 준 케이스와, 무작위로 섞어서 추천을 진행한 결과에 큰 차이가 없음. (SASRec같은 전통적인 순차 추천과는 대조적)
    
- Input sequence가 랜덤성 요소로 인해 방해받을 때 모델로부터 얻는 사용자 표현에 크게 차이가 존재하지 않음.
    
    → Original, Shuffle(random) 사용자 표현 유사도 측정한 결과, 크게 차이가 없어 순차 정보를 완전히 잡아내지 못하는 것을 알 수 있음.
    

> **Limitations의 근거가 되는 실험**
> 
- Next Item Title Generation, Next Item Retrieval 2개 방식 모두 진행
    - **생성적 접근(Next Item Title Generation)**: 사용자의 상호작용 기록과 후보 아이템 목록을 텍스트 프롬프트로 LLM에 입력하여 다음 아이템 제목을 생성
    - **검색적 접근(Next Item Retrieval)**: LLM으로부터 사용자·아이템 표현을 뽑아내고, 이들 간 유사도로 추천 항목을 검색

<img width="541" height="616" alt="image" src="https://github.com/user-attachments/assets/f629c1da-0350-403b-8113-4a5e0834a073" />


(Next Item Retrieval)

<img width="541" height="514" alt="image" src="https://github.com/user-attachments/assets/4eba193d-6859-4c22-88ca-05e4fd213f5c" />


(Next Title Generation)

**Shuffled Training**

- Original(non-shuffled), Shuffled 시퀀스를 구분하여 학습 및 테스트 진행
- 두 방식의 결과를 모두 볼 때, Sequential Recommendation**(CF-SRec)**인 SASRec은 성능하락의 차이가 확실한 반면, LLM을 활용하는 추천들은 차이가 적고, 오히려 일부 케이스는 성능 향상이 이루어진 것을 알 수 있음

→ **LLM이 순차 정보를 제대로 활용하고 있지 않다는 것을 알 수 있음. (Interaction sequence에서 순차 정보 캡쳐하는 것이 어려움)**

**Shuffled Inference**

<img width="1094" height="718" alt="image" src="https://github.com/user-attachments/assets/8ce93749-070b-4700-8d98-b328cab7d082" />


- 테스트 과정에서 시퀀스를 Shuffle해서 평가
- 테스트 결과, SASRec은 추론 시퀀스를 섞으면 성능이 크게 떨어지는 반면, 모든 LLM4Rec 모델은 셔플된 입력에도 성능 차이가 없음(위의 표 참고)
- 이는 LLM4Rec 모델이 순차 정보 변화에 둔감하게 반응함을 보여줌 (순차 정보를 잘 캡쳐하지 못함)

**Representation Similarity**

<img width="513" height="267" alt="image" src="https://github.com/user-attachments/assets/7713dd3a-d2ce-4323-9361-b20e3cfb78cb" />


- Original, Shuffle 사용자 표현 간의 코사인 유사도 비교
- SASRec은 유사도가 약 0.65∼0.75로 비교적 낮아, 시퀀스 변화에 따라 표현이 크게 달라짐
- LLM4Rec 모델들은 대부분 0.88∼0.98 수준으로 매우 높은 유사도를 보여, 시퀀스 순서가 바뀌어도 표현이 거의 변하지 않음을 알 수 있음

**→ LLM based Recommendation 모델들이 순차 정보를 잘 인식하지 못한다는 것을 확인**

## Method : LLM-SRec

<img width="745" height="738" alt="image" src="https://github.com/user-attachments/assets/1c0b6def-132a-4fad-bc65-2d964275f749" />


- LLM에 순차 정보를 잘 포함하도록 하는 LLM4Rec 모델
- Next Item Retrieval 방식

### Distilling Sequential Information

- 사전학습되고, 고정된 CF-SRec을 활용해 순차적 지식을 LLM에 증류함.
- 사전학습된 CF-SRec에서 생성한 사용자 표현, LLM에서 생성한 사용자 표현을 각각 2-layer MLP로 투영시킨 다음, MSE로 대응시킴
    - $\text{O}_u$ : CF-SRec based user representation, $\text{h}_u$ : LLM-based user representation
    - $f_{\text{CF-user}}, f_{\text{user}}$ : 2-layer MLP 구조의 function (Trainable)

<img width="551" height="58" alt="image" src="https://github.com/user-attachments/assets/304de588-e43c-4df8-b167-7535c6b4e9e7" />


- 이 과정을 통해 LLM이 CF-SRec의 순차적 의존성을 내재화하도록 함 (순차 정보를 인지하도록)

### Preventing Over-smoothing

<img width="607" height="135" alt="image" src="https://github.com/user-attachments/assets/9c3a43b6-c258-4955-ba3f-af4d23090d49" />


- 단순 MSE 증류만 적용할 경우 서로 다른 사용자 표현이 지나치게 유사해질 수 있는 **over-smoothing** 문제가 발생할 수 있음.
- 이러한 Over-smoothing을 해결하기 위해 **Uniformity Loss** 사용
    - 표현 간의 거리를 유지하면서 균등 분포 유발
    - 다른 사용자의 사용자 표현이 정규화된 특성공간에서 균일하게 분포되도록 보장해 분리성(separation), 정보성(informativeness) 모두 유지
    - 서로 다른 사용자 벡터 쌍이 서로 멀어지도록 함.

**Retrieval Loss**

<img width="509" height="120" alt="image" src="https://github.com/user-attachments/assets/4d6c3818-9e00-46dc-920e-6170e51a1bb1" />


- CF-SRec 모델에서의 Loss
    - $f_{\text{item}}, f_{\text{user}}$ : 2-layer MLP
- Retrieval 방식으로 추천을 수행하고, 사용자 $u$와 후보 아이템 $i$의 유사도 점수를 계산해서 $s(u,i)$로 계산
- Loss는 softmax-cross-entropy 방식으로 계산해 positive item의 유사도가 최대화하도록 함.

**Final Objective**

<img width="566" height="70" alt="image" src="https://github.com/user-attachments/assets/ec9b8fb8-a1f2-44c3-a5e9-28743bbb4f92" />


- 앞서 구한 3개의 Loss를 합한 지표로 최종 Loss function 정의
- LLM-SRec은 모든 부분을 학습하는 것이 아닌, MLP-layer 세트($f_{\text{I}}, f_{\text{user}}, f_{\text{CF-user}}, f_{\text{item}}$)와 2개의 Special Token([UserOut], [ItemOut])만 학습
    - $f_{\text{I}}$ : LLM이 기대하는 차원으로 변환시켜주는 2-layer MLP

## Experiments

### Experiments Settings

<img width="619" height="187" alt="image" src="https://github.com/user-attachments/assets/9738d9a6-33c0-469b-a574-2d05394b9e66" />


- leave-one-out 사용
- Full ranking x, 99개의 negative 데이터 랜덤하게 선택해서 테스트 진행
- NDCG, Hit Ratio 사용
- LLM : Llama 3.2-3B-Instruct 모델 사용 (비교 실험 있음 vs Llama 3.1-8B)
- CF-SRec : SASRec 사용

### Overall Performance

<img width="1265" height="506" alt="image" src="https://github.com/user-attachments/assets/cbbd7638-c289-4ac5-88f8-26acb3bb9ee5" />


- **LLM-SRec**이 네 개 도메인 전반에서 기존 LLM4Rec 모델들뿐 아니라 CF-SRec, LM 기반 모델들을 일관되게 능가함
- LLaRA·CoLLM·A-LLMRec 등 CF-SRec 정보를 활용한 기존 모델들은 TALLRec 대비 개선을 보였으나, 순차 의존성 학습의 부재로 한계

→ CF-SRec에서 증류한 순차 정보를 LLM에 통합하는 것이 유의미한 성능 증가를 보임

**Transition & Non-Transition Sequences**

<img width="1472" height="664" alt="image" src="https://github.com/user-attachments/assets/516a4947-7e4d-4d6b-a112-215a9ca8df5e" />


- 사용자 시퀀스 내 연속 아이템 전이 개수 비율을 기준으로 상위 50%를 Transition, 나머지를 Non-Transition으로 설정
- 두 케이스 모두 LLM-SRec이 우수한 성능을 보임

**Cold-Warm Scenarios**

<img width="1376" height="570" alt="image" src="https://github.com/user-attachments/assets/eefd87ff-8e81-4227-af01-68dc2db45f49" />


- Interaction 기준 상위 35% 아이템을 Warm, 하위 35%를 Cold로 지정
- 기존 LM 기반 추천에서는 Warm start 부분에서 약세를 보이고, SASRec같은 CF-SRec의 경우는 Cold-start에서 약세를 보임

**Cross-Domain Scenarios**

<img width="1350" height="324" alt="image" src="https://github.com/user-attachments/assets/a10195fa-4493-4943-bd4e-089eb9fe83b8" />


- Electronics 데이터셋으로 학습한 다음, Scientific, CDs 데이터셋으로 평가
- LLM-SRec은 학습에 사용하지 않은 도메인에서도 모든 베이스라인을 상회하며, 텍스트 이해와 증류된 순차 지식을 효과적으로 결합함을 알 수 있음.

### Ablation Study

<img width="1233" height="373" alt="image" src="https://github.com/user-attachments/assets/3c063781-ef55-48b8-b8f1-24b337388e8f" />


- (a) :  Distill Loss, Uniformity Loss 모두 제거
- (b) : Uniformity Loss만 제거
- (c) : Original Model
- 원본 학습 시퀀스로 학습시키고, 셔플된 테스트 데이터에서 결과 확인
    - (a)의 경우는, 성능 차이가 거의 없음 → 이로 보아, 순차 정보가 제대로 학습되지 않는 것을 알 수 있음
    - (b)의 경우는, 성능은 (a)대비 차이가 있지만, 과도한 평탄화로 인해서 성능의 한계가 있다 언급
    - (c)는 원본 테스트 중 성능이 가장 높고, 셔플 시에도 가장 큰 감소폭을 보임

→ Distill Loss가 순차 정보를 인식하는 것, Uniformity Loss가 평탄화 방지에 핵심적인 기여를 함을 알 수 있음.

### Model Analysis

**Train/Inference time comparison**

<img width="1050" height="430" alt="image" src="https://github.com/user-attachments/assets/192a4fc8-d0d3-4b47-b908-18648765f673" />


- LLM-SRec이 파인튜닝하는 것이 아닌, MLP와 특수 토큰만 학습을 하기에 학습과 추론 모두 빠름
- A-LLMRec보다도 효율적인데, 2단계 학습으로 인해 프롬프트 길이가 늘어났기 때문이라고 언급

**Size of LLM**

<img width="1068" height="378" alt="image" src="https://github.com/user-attachments/assets/5e629feb-ca1a-491e-a305-df5ccae59164" />


- LLM을 LLaMA 3.1-8B로 교체했을 때, 모델 성능이 향상되지만 크게 차이가 존재하지 않음을 알 수 있음.
- 단순히 모델 크기를 키우는 것 보다, 순차 정보를 포함하도록 하는 것이 더 중요함을 알 수 있음.

**Case Study**

<img width="1072" height="740" alt="image" src="https://github.com/user-attachments/assets/af89678e-97aa-47ac-bbb6-9b6e9ca8747e" />


- Electronics 데이터에서 추천 품질 비교
- 이를 통해 텍스트 이해와 순차 정보의 통합이 얼마나 좋은 성능을 내는지 알 수 있음.

## Contributions, Limitations

**Contributions**

- 기존 LLM4Rec 모델의 순차 정보 인식에 관련된 문제를 해결 → 상호작용 시퀀스를 셔플링한 것과, 원본 시퀀스 간의 차이가 없다는 것을 인지하고, 이에 대한 근본적인 해결책을 제안함
- 적은 학습비용으로 효율적인 성능을 보여줌
- 순차 추천 이외에도 다양한 평가 시나리오에서 좋은 성능
    - Cold/Warm, Cross-Domain, transition/non-transition, in-domain에서 모두 가장 좋은 성능을 보임
- 사용자, 아이템 관점을 모두 고려한 추천 시스템

**Limitations**

- LLM 기반 추천 모델인데도, 순차 추천 모델에 대한 의존성이 큼.
- Full negative로 했을 때 성능이 증명되지 않음. → 다양한 평가 시나리오 중에 이 Full negative가 포함되었더라면…?
  - 추천 논문들 중에서는 Full negative로 진행하는 케이스가 많음
